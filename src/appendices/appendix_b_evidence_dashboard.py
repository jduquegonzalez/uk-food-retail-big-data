"""
Appendix B.2: Evidence Base Validation Dashboard

Four-panel dashboard visualising the extraction matrix composition and
theoretical framework alignment for methodological rigour assessment.

Data Source:
    Secondary Data Extraction Matrix v2.3
    N-gram Analysis Results

Author: Jonathan Duque González
Version: 2.3
"""

import os
import random
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from wordcloud import WordCloud

# ═══════════════════════════════════════════════════════════════════════════════
# PATH CONFIGURATION
# ═══════════════════════════════════════════════════════════════════════════════

# Get the directory where this script is located
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# Navigate to repository root (two levels up from src/appendices/)
REPO_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))
# Set output and data directories relative to repo root
OUTPUT_DIR = os.path.join(REPO_ROOT, 'outputs')
DATA_DIR = os.path.join(REPO_ROOT, 'data')

# Visual settings
COLORS_SOURCE = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#7A7A7A']
COLORS_FRAMEWORKS = {'TOE': '#E8505B', 'SCOR': '#F9C74F', '5Vs': '#14A76C'}
COLORS_WORDCLOUD = ['#2E86AB', '#A23B72', '#14A76C', '#F18F01', '#E8505B', '#6B4C9A']

# Data mappings
SOURCE_MAPPING = {
    'Retailer': 'Retailers', 'Consultancy': 'Consultancy',
    'Trade_Association': 'Trade/NGO', 'NGO': 'Trade/NGO',
    'Government': 'Government', 'News': 'News/Other',
    'Vendor': 'News/Other', 'Trade_Publication': 'News/Other',
    'Academic': 'News/Other'
}

THEME_LABELS = {
    'TH_01': 'TH_01: Operational Efficiency',
    'TH_02': 'TH_02: Cost Reduction',
    'TH_03': 'TH_03: Adoption Barriers',
    'TH_04': 'TH_04: Implementation Enablers',
    'TH_05': 'TH_05: Strategic Value',
    'TH_06': 'TH_06: External Environment'
}

THEME_ORDER = ['TH_01', 'TH_02', 'TH_03', 'TH_04', 'TH_05', 'TH_06']
SOURCE_ORDER = ['Retailers', 'Consultancy', 'Trade/NGO', 'Government', 'News/Other']

FRAMEWORK_CODES = {
    'TOE': ['TOE_Tech', 'TOE_Org', 'TOE_Env'],
    'SCOR': ['SCOR_Plan', 'SCOR_Source', 'SCOR_Make', 'SCOR_Deliver', 'SCOR_Return'],
    '5Vs': ['5Vs_Volume', '5Vs_Velocity', '5Vs_Variety', '5Vs_Veracity', '5Vs_Value']
}

FRAMEWORK_LABELS = {
    'TOE': ['Technology', 'Organisation', 'Environment'],
    'SCOR': ['Plan', 'Source', 'Make', 'Deliver', 'Return'],
    '5Vs': ['Volume', 'Velocity', 'Variety', 'Veracity', 'Value']
}


def load_data(matrix_path=None, ngram_path=None):
    """Load extraction matrix and n-gram data."""
    if matrix_path is None:
        matrix_path = os.path.join(DATA_DIR, 'Secondary_Data_Extraction_Matrix_v2.3.xlsx')
    if ngram_path is None:
        # N-gram file is generated by appendix_b3_ngram_analysis.py into outputs/
        ngram_path = os.path.join(OUTPUT_DIR, 'Ngram_Analysis.xlsx')

    df = pd.read_excel(matrix_path, sheet_name='Extraction_Log')
    df_inc = df[df['Reviewer_Decision'] == 'Include'].copy()

    df_ngrams = pd.read_excel(ngram_path, sheet_name=None)
    df_unigrams = df_ngrams['Unigrams']
    df_bigrams = df_ngrams['Bigrams']
    df_trigrams = df_ngrams['Trigrams']

    return df_inc, df_unigrams, df_bigrams, df_trigrams


def generate_figure(df_inc, df_unigrams, df_bigrams, output_path=None):
    """Generate the four-panel evidence validation dashboard."""

    # Data preparation
    df_inc['Source_Group'] = df_inc['Source_Type'].map(SOURCE_MAPPING)
    cross = pd.crosstab(df_inc['Theme_Candidate'], df_inc['Source_Group'])
    cross = cross.reindex(THEME_ORDER, fill_value=0)
    cross = cross[SOURCE_ORDER]

    framework_counts = df_inc['Framework_Alignment'].value_counts()
    framework_data = {}
    for fw_name, codes in FRAMEWORK_CODES.items():
        framework_data[fw_name] = [framework_counts.get(code, 0) for code in codes]

    org_count = df_inc['Organisation'].nunique()

    fig = plt.figure(figsize=(16, 14))
    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.25)

    # Panel A: Evidence Distribution
    ax1 = fig.add_subplot(gs[0, 0])
    y_pos = np.arange(len(cross))
    left = np.zeros(len(cross))

    for i, col in enumerate(cross.columns):
        ax1.barh(y_pos, cross[col], left=left, color=COLORS_SOURCE[i],
                 edgecolor='white', linewidth=0.5, label=col)
        left += cross[col].values

    ax1.set_yticks(y_pos)
    ax1.set_yticklabels([THEME_LABELS[t] for t in cross.index], fontsize=9)
    ax1.set_xlabel('Number of Extracts', fontsize=10, fontweight='light')
    ax1.set_title(f'Panel A: Evidence Distribution by Theme & Source\n'
                  f'(n={cross.sum().sum():.0f} extracts, {org_count} organisations)',
                  fontsize=11, fontweight='bold')
    ax1.legend(fontsize=8, loc='lower right', title='Source Type', title_fontsize=8)
    ax1.invert_yaxis()
    ax1.set_xlim(0, cross.sum(axis=1).max() * 1.15)
    ax1.spines[['top', 'right']].set_visible(False)

    for i, total in enumerate(cross.sum(axis=1)):
        ax1.text(total + 1, i, f'n={int(total)}', va='center', fontsize=8, fontweight='bold')

    # Panel B: Framework Alignment
    ax2 = fig.add_subplot(gs[0, 1])
    all_values = (framework_data['TOE'] + framework_data['SCOR'] + framework_data['5Vs'])
    all_labels = (FRAMEWORK_LABELS['TOE'] + FRAMEWORK_LABELS['SCOR'] + FRAMEWORK_LABELS['5Vs'])
    all_colors = ([COLORS_FRAMEWORKS['TOE']] * 3 +
                  [COLORS_FRAMEWORKS['SCOR']] * 5 +
                  [COLORS_FRAMEWORKS['5Vs']] * 5)
    y_pos = [0, 1, 2, 3.5, 4.5, 5.5, 6.5, 7.5, 9, 10, 11, 12, 13]

    ax2.barh(y_pos, all_values, color=all_colors, height=0.7,
             edgecolor='white', linewidth=0.5)

    for y, v in zip(y_pos, all_values):
        if v > 0:
            ax2.text(v + 0.5, y, str(int(v)), va='center', fontsize=8, fontweight='bold')

    ax2.set_yticks(y_pos)
    ax2.set_yticklabels(all_labels, fontsize=9)
    ax2.set_xlabel('Number of Extracts', fontsize=10, fontweight='light')
    ax2.set_title('Panel B: Theoretical Framework Alignment\n(TOE, SCOR, 5Vs Coverage)',
                  fontsize=11, fontweight='bold')
    ax2.invert_yaxis()
    ax2.set_xlim(0, max(all_values) * 1.15)
    ax2.spines[['top', 'right']].set_visible(False)

    # Panel C: N-gram Comparison
    ax3 = fig.add_subplot(gs[1, 0])
    TOP_N = 8
    uni_top = df_unigrams.head(TOP_N)
    bi_top = df_bigrams.head(TOP_N)
    x = np.arange(TOP_N)
    width = 0.35

    bars1 = ax3.bar(x - width/2, uni_top['Frequency'], width,
                    label='Unigrams', color='#2E86AB', edgecolor='white')
    bars2 = ax3.bar(x + width/2, bi_top['Frequency'], width,
                    label='Bigrams', color='#A23B72', edgecolor='white')

    labels_combined = []
    for i in range(TOP_N):
        uni = uni_top.iloc[i]['Unigram']
        bi = bi_top.iloc[i]['Bigram'].replace(' ', '\n')
        labels_combined.append(f"{uni}\n|\n{bi}")

    ax3.set_xticks(x)
    ax3.set_xticklabels(labels_combined, fontsize=8)
    ax3.set_ylabel('Frequency', fontsize=10, fontweight='bold')
    ax3.set_title('Panel C: Top Unigrams vs Bigrams\n(Linguistic Pattern Validation)',
                  fontsize=11, fontweight='bold')
    ax3.legend(fontsize=9, loc='upper right')
    ax3.spines[['top', 'right']].set_visible(False)

    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2, height + 1,
                     str(int(height)), ha='center', va='bottom', fontsize=8, fontweight='bold')

    # Panel D: Word Cloud
    ax4 = fig.add_subplot(gs[1, 1])
    bigram_freq = dict(zip(df_bigrams['Bigram'], df_bigrams['Frequency']))

    def custom_color(*args, **kwargs):
        return random.choice(COLORS_WORDCLOUD)

    wc = WordCloud(
        width=800, height=500, background_color='white',
        max_words=40, max_font_size=80, min_font_size=10,
        prefer_horizontal=0.7, collocations=False,
        random_state=42, color_func=custom_color
    ).generate_from_frequencies(bigram_freq)

    ax4.imshow(np.array(wc.to_image()), interpolation='bilinear')
    ax4.axis('off')
    ax4.set_title('Panel D: Bigram Word Cloud\n(Concept Prominence Visualisation)',
                  fontsize=11, fontweight='bold')

    # Titles
    plt.suptitle(
        'Evidence Base Validation Dashboard\n'
        'Secondary Data Extraction Matrix v2.3 — Methodological Rigour Assessment',
        fontsize=14, fontweight='bold', y=0.99
    )

    plt.figtext(
        0.5, 0.01,
        f'Source: {len(df_inc)} extracts from {org_count} organisations (2019–2025). '
        'Thematic coding follows Braun & Clarke (2006). '
        'N-gram analysis validates keyword convergence with thematic framework.',
        ha='center', fontsize=9, style='italic', wrap=True
    )

    plt.tight_layout()

    if output_path:
        for fmt in ['png', 'pdf']:
            filepath = f'{output_path}.{fmt}'
            fig.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')
            print(f"✓ Saved: {filepath}")

    return fig


def main():
    """Generate Appendix B dashboard."""
    print("\n" + "="*60)
    print("APPENDIX B.2: Evidence Base Validation Dashboard")
    print("="*60)

    try:
        df_inc, df_unigrams, df_bigrams, _ = load_data()
        print(f"\n✓ Loaded {len(df_inc)} extracts, "
              f"{len(df_unigrams)} unigrams, {len(df_bigrams)} bigrams")

        output_path = os.path.join(OUTPUT_DIR, 'appendix_b_evidence_dashboard')
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        generate_figure(df_inc, df_unigrams, df_bigrams, output_path)

        print("\n✓ Dashboard generation complete.\n")

    except FileNotFoundError as e:
        print(f"\n✗ Data file not found: {e}")
        print("  Required files:")
        print(f"  - {os.path.join(DATA_DIR, 'Secondary_Data_Extraction_Matrix_v2.3.xlsx')}")
        print(f"  - {os.path.join(OUTPUT_DIR, 'Ngram_Analysis.xlsx')}")
        print("\n  Run appendix_b3_ngram_analysis.py first to generate N-gram data.\n")


if __name__ == "__main__":
    main()




